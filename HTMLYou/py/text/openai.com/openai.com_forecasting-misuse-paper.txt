



[2301.04246] Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations

















































Global Survey
In just 3 minutes help us understand how you see arXiv.



TAKE SURVEY





Skip to main content






We gratefully acknowledge support fromthe Simons Foundation and member institutions.





 > cs > arXiv:2301.04246
  





Help | Advanced Search




All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text




Search















open search






GO



open navigation menu


quick links

Login
Help Pages
About













Computer Science > Computers and Society


arXiv:2301.04246 (cs)
    




  
  
  
    
  
  
    
    
  

  [Submitted on 10 Jan 2023]
Title:Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations
Authors:Josh A. Goldstein (1 and 3), Girish Sastry (2), Micah Musser (1), Renee DiResta (3), Matthew Gentzel (2), Katerina Sedova (1) ((1) Georgetown's Center for Security and Emerging Technology, (2) OpenAI, (3) Stanford Internet Observatory)
Download PDF

Abstract:  Generative language models have improved drastically, and can now produce
realistic text outputs that are difficult to distinguish from human-written
content. For malicious actors, these language models bring the promise of
automating the creation of convincing and misleading text for use in influence
operations. This report assesses how language models might change influence
operations in the future, and what steps can be taken to mitigate this threat.
We lay out possible changes to the actors, behaviors, and content of online
influence operations, and provide a framework for stages of the language
model-to-influence operations pipeline that mitigations could target (model
construction, model access, content dissemination, and belief formation). While
no reasonable mitigation can be expected to fully prevent the threat of
AI-enabled influence operations, a combination of multiple mitigations may make
an important difference.

    




Comments:
82 pages, 26 figures


Subjects:

Computers and Society (cs.CY)

Cite as:
arXiv:2301.04246 [cs.CY]


 
(or 
arXiv:2301.04246v1 [cs.CY] for this version)
          


 

https://doi.org/10.48550/arXiv.2301.04246



Focus to learn more




                arXiv-issued DOI via DataCite
              







Submission history From: Girish Sastry [view email]
      [v1]
Tue, 10 Jan 2023 23:42:30 UTC (16,883 KB)





Full-text links:
Download:

PDF
Other formats
(license)



    Current browse context: cs.CY


< prev

  |  

next >


new
 | 
recent
 | 
2301

    Change to browse by:
    
cs




References & Citations

NASA ADSGoogle Scholar
Semantic Scholar




a
export bibtex citation
Loading...




Bibtex formatted citation
×


loading...


Data provided by: 





Bookmark

















Bibliographic Tools

Bibliographic and Citation Tools






Bibliographic Explorer Toggle



Bibliographic Explorer (What is the Explorer?)







Litmaps Toggle



Litmaps (What is Litmaps?)







scite.ai Toggle



scite Smart Citations (What are Smart Citations?)








Code, Data, Media

Code, Data and Media Associated with this Article






Links to Code Toggle



Papers with Code (What is Papers with Code?)







ScienceCast Toggle



ScienceCast (What is ScienceCast?)








Demos

Demos






Replicate Toggle



Replicate (What is Replicate?)







Spaces Toggle



Hugging Face Spaces (What is Spaces?)







Related Papers

Recommenders and Search Tools






Connected Papers Toggle



Connected Papers (What is Connected Papers?)







Core recommender toggle



CORE Recommender (What is CORE?)








        About arXivLabs
      



arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.










Which authors of this paper are endorsers? |
    Disable MathJax (What is MathJax?)
    












About
Help





contact arXivClick here to contact arXiv
 Contact


subscribe to arXiv mailingsClick here to subscribe
 Subscribe











Copyright
Privacy Policy




Web Accessibility Assistance


arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack





 






